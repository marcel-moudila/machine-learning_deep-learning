{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligence Artificielle Avancée\n",
    "## TP2 : Introduction au Deep Learning\n",
    "\n",
    "Au cas où, il nous faut d'abord vérifier la version du Keras et Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "print(\"Keras:\", keras.__version__)\n",
    "print(\"Tensorflow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installer Pydot et Graphiz, si ne sont pas déjà installés:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Si vous utilisez Anaconda, tapez: \"!conda install pydot\" sinon, \"!pip install pydot\" \n",
    "Ou vous pouvez également installer Pydot à l'aide de l'interface graphique Anaconda:\n",
    "Menu--> Environnements-->Not installed, puis cherchez pydot, cliquez \"Apply\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis vous devez relancer le noyau : trouver 'restart kernel' dans les menus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De la documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning avec Keras\n",
    "- Doc keras :https://keras.io/\n",
    "- Le model Sequential (premier pas) : https://keras.io/getting-started/sequential-model-guide/#getting-started-with-the-keras-sequential-model\n",
    "- Un framework plus riche : https://keras.io/getting-started/functional-api-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autres Toolkits et packages\n",
    "\n",
    "- Lasagne : Langage *de haut niveau* comme keras\n",
    "\n",
    "\n",
    "NB : Lasagne et Keras utilisent indifférement un backend parmi Theano (Univ. Montreal) et Tensorflow (Google)\n",
    "\n",
    "- Theano :  le package de l'Univ. de Montréal\n",
    "\n",
    "- Tensorflow :  le package Google\n",
    "\n",
    "- Caffe : très spécialisé images\n",
    "\n",
    "- Pytorch : la plateforme de Facebook\n",
    "\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération d'un dataset existant\n",
    "On va travailler avec un jeux de données historique que vous conaissez déjà : MNIST. Ce sont des chiffres manuscrits, donc des images en niveaux de gris, en résolution 28x28 cette fois (cela peut prendre un peu de temps - et générer un FutureWarning, ce qui n'est pas grave) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"dimension des données : \", X_train.shape, y_train.shape)\n",
    "print(\"une données : \")\n",
    "print(X_test[42])\n",
    "\n",
    "print(\"Une données sous format plus lisible :\")\n",
    "plt.imshow(X_train[42], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitement\n",
    "Les données ne sont pas dans le format nécessaire à Keras : il vaut vectoriser chaque image. On va aussi les normaliser, pour n'avoir que des valeurs comprisent entre 0 et 1 (améliore la vitesse de convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "import numpy as np\n",
    "X_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
    "X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(\"Classe de la data num 42 :\", y_train[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Réseaux de neurones </h2>\n",
    "\n",
    "Utilisons maintenant <i>keras</i> pour instancier et évaluer des réseaux de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot-encoding\n",
    "Les réseaux de neurones ont besoin d'un vecteur à la place de *y* : on utilise un codage où la taille de ce vecteur est le nombre de classe, toutes les valeurs du vecteurs sont égales à 0, sauf celle qui correspond à la bonne classe qui elle vaut 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On transforme les sorties (numéros de classe) en des vecteurs de type one-hot-code\n",
    "nb_classes = 10\n",
    "\n",
    "Y_train = y_train\n",
    "Y_test = y_test\n",
    "y_train =  np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(\"Classe de la data num 42 : \", y_train[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par un réseau très simple (un perceptron multiclasses), équivalent à une régression logistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from Keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis il faut \"compiler\" le modèle, en précisant :\n",
    "- le critère d'optimisation : le *loss*\n",
    "- la routine d'optimisation (ie l'utilisation du gradient) : l'*optimizer*\n",
    "- les métriques additionnelles au *loss* (ici l'*accuracy*, le taux de bonne classification) que l'on va calculer à chaque fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "num_classes = 10\n",
    "input_shape = (784,)\n",
    "sgd = SGD(lr=0.01)\n",
    "# Model\n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apprentissage et évaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.fit(X_train, y_train, batch_size=128, epochs=20, validation_split=0.8, verbose=0)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"score=\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Historique de processus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.plot(loss.history['loss'], label=\"loss\")\n",
    "plt.plot(loss.history['val_loss'], label=\"val_loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Identifier le surapprentissage </h3>\n",
    "\n",
    "Lorsque la courbe \"loss\" décroit mais que \"val_loss\" ne suit pas, c'est que le réseau surapprend (overfitting). Plusieurs techniques de régularisation permettent d'éviter ces situations (à voir en cours) : augmenter la taille du minibatch peut éviter l'overfitting tout en accélerant l'apprentissage, mais au dépend d'une réduction de la précision lorsque le minibatch est trop grand. A noter aussi qu'il faut aussi un espace mémoire (RAM ou GPU) suffisant pour charger le minibatch en plus des paramètres du réseau.\n",
    "\n",
    "<h2>A faire : </h2> Augmentez le minibatch et commentez les courbes obtenues. Attention, si vous appelez la fonction <i>fit</i> sans réinstancier le modèle (la cellule d'avant..), l'entrainement continue où il en était et faussera vos conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprendre un réseau plus \"profond\"\n",
    "D'abord il faut créer le model.\n",
    "On ajoute les couches une à une. Ici un modèle qui :\n",
    "- prend en entrée un vecteur de dimension 784 (une image Mnist 28x28 vectorisée)\n",
    "- transforme l'entrée en un vecteur de dimension 64 avec une couche totalement connectée (Dense), avec une fonction d'une activation de type Rectified Linear Unit \n",
    "- Transforme la sortie de la couche précédente (de dimension 64) en un vecteur de dimension 10 avec une autre couche dense \n",
    "- Transforme le vecteur de dimenbsion 10 en un autre vecteur de dimension 10 à l'aide d'une couche dense avec la fonction d'activation softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from Keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée note réseau de neurones (cela génère un warning désagréable mais qui ne pause pas de problème finalement) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=784, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis il faut \"compiler\" le modèle, en précisant :\n",
    "- le critère d'optimisation : le *loss*\n",
    "- la routine d'optimisation (ie l'utilisation du gradient) : l'*optimizer*\n",
    "- les métriques additionnelles au *loss* (ici l'*accuracy*, le taux de bonne classification) que l'on va calculer à chaque fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage de la structure du modèle\n",
    "\n",
    "Expliquer les différentes éléments de chacune des lignes affichées par la commande suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou une manière plus graphique (nécessite l'installation de pydot et surtout de graphviz, ce qui n'est pas forcément facile) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On peut maintenant apprendre le modèle en précisant \n",
    "- la base d'apprentissage (les deux premiers paramètres)\n",
    "- le nombre d'itérations d'apprentissage (*epochs*)\n",
    "- la taille des minibatchs (*batch_size*)\n",
    "- un ensemble de validation (soit on utilise comme ici un pourcentage des données d'entrée *X_train, y_train*, soit d'autres ensemble de données via *validation_data=(X_test, y_test)*)\n",
    "- le niveau de verbosité de l'affichage\n",
    "\n",
    "*(là encore, on a un WARNING, mais il n'empêche pas le code de fonctionner)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit(X_train, y_train,\n",
    "              epochs=20,\n",
    "              batch_size=16,\n",
    "              verbose =1,\n",
    "              validation_split=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "On peut alors évaluer le modèle sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=2, batch_size=16)\n",
    "\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historique de processus\n",
    "Les info sur le processus d'apprentissage sont stockées dans *h*.\n",
    "\n",
    "Comme on va en avoir besoin plusieurs fois, on écrit une fonction qui prend un historique d'apprentissage et affiche les courbes : une pour la fonction de perte (loss) et l'autre pour le taux de réussite (accuracy) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affiche_evolution_apprentissage(history):\n",
    "    plt.figure()\n",
    "    #affiche history.history.keys()\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('accuracy du modèle')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['données apprentissage', 'données test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # résumé de l'historique pour loss\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('loss du modèle')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['apprentissage', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affiche_evolution_apprentissage(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant tensorflow en backend, vous avez aussi accès au [tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard) (que ceux qui le veulent y jette un coup d'oeil)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback\n",
    "Permet de programmer la sauvegarde des modèles à chaque itération, l'adaptation du pas d'apprentissage (learning rate), une procédure de early stopping, etc.\n",
    "\n",
    "Voir <https://keras.io/callbacks/> pour les détails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "# Adaptation du pas d'apprentissage\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=0, mode='auto', \n",
    "                       min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "h_es_lr = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=16,\n",
    "                    verbose =1,\n",
    "                    validation_split=0.33,\n",
    "                    callbacks=[es, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauver et récupérer des models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Eventuellement, installation d'un module nécessaire :\n",
    "#!pip3 install h5py\n",
    "\n",
    "score = model.evaluate(X_test, y_test, batch_size=16)\n",
    "print (\"Initialement : \", score)\n",
    "\n",
    "# Sauver le model \n",
    "model.save('mon_modele.h5')  # crée un fichier HDF5del model  \n",
    "\n",
    "# supprime le modèle\n",
    "del model\n",
    "\n",
    "# Récupérer le modèle   \n",
    "model = load_model('mon_modele.h5')\n",
    "\n",
    "score = model.evaluate(X_test, y_test, batch_size=16)\n",
    "print (\"Après suppression-récupération : \", score)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser Keras dans un code SciKit-learn (cross-validation / Grid search)\n",
    "\n",
    "Dans Scikit-Learn, il y a une classe *KerasClassifier* qu'il peut-être utile de savoir utiliser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_model_Mnist(optimizer='rmsprop', input_datadim = 784, init='glorot_uniform', nb_hid1= 20, do_rate= 0.5 ):\n",
    "    # fonction créant un modèle pour MNIST\n",
    "    \"\"\"\n",
    "    #Jusqu'à présent :\n",
    "    m = Sequential()\n",
    "    m.add(Dense(nb_hid1, input_dim=input_datadim, activation='relu'))\n",
    "    m.add(Dropout(do_rate))\n",
    "    m.add(Dense(64, activation='relu'))\n",
    "    m.add(Dropout(do_rate))\n",
    "    m.add(Dense(10, activation='softmax'))\n",
    "    \"\"\"\n",
    "    # De façon équivalente :\n",
    "    entree= Input(shape=(784,))\n",
    "    cachee_1= Dense(64, activation='relu')(entree)\n",
    "    cachee_2 = Dense(64, activation='relu')(cachee_1)\n",
    "    sortie = Dense(10, activation=\"softmax\")(cachee_2)\n",
    "    m = Model(entree, sortie)\n",
    "    \n",
    "    m.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    m.summary()\n",
    "    return m\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model_Mnist)\n",
    "\n",
    "# valeurs des différents paramètres\n",
    "optimizers = ['adam', 'rmsprop']\n",
    "init = ['glorot_uniform']\n",
    "V_nb_hid1 = [100]\n",
    "DO_rate=[0, 0.5]\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "param_grid = dict(optimizer=optimizers, init=init,  nb_hid1=V_nb_hid1, do_rate= DO_rate)\n",
    "\n",
    "print(param_grid)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, epochs = epochs, verbose=2)\n",
    "\n",
    "# Résumé des résultats\n",
    "print(\"-----------------------------\")\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des vecteurs d'activation d'une couche\n",
    "Une fois un réseau appris, on peut avoir besoin des vecteurs de sortie d'une couche cachée. Avec Keras, il suffit de créer un nouveau modèle qui ne contient que le début du réseau jusqu'à la couche dont on veut accéder aux sorties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#réseau complet\n",
    "entree= Input(shape=(784,))\n",
    "cachee_1= Dense(64, activation='relu')(entree)\n",
    "cachee_2 = Dense(64, activation='relu')(cachee_1)\n",
    "sortie = Dense(10, activation=\"softmax\")(cachee_2)\n",
    "m = Model(entree, sortie)\n",
    "\n",
    "#apprentissage\n",
    "m.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "m.fit(X_train, y_train, epochs=2, batch_size=16, verbose =1, validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#réseau partiel\n",
    "m2 = Model(entree,cachee_1)\n",
    "\n",
    "#On aurait aussi pu faire :\n",
    "# m2 = Sequential()\n",
    "# m2.add(Dense(64,input_dim=784, activation='relu', weights=m.layers[0].get_weights()))\n",
    "\n",
    "#récupération d'une matrice d'activation :\n",
    "#chaque ligne est le vecteur de sortie de la dernière couche de m2 pour la donnée correspondante\n",
    "activite = m2.predict(X_test)\n",
    "print(activite.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A faire\n",
    "Choisissez l'une de ces possibilités et réalisez la avec panache et déterminisation !\n",
    "\n",
    "## Possibilité 1 : utiliser un réseau pour obtenir une autre représentation des données\n",
    "- Apprendre un auto-encoder pour transformer les données \n",
    "- Utiliser un classifieur standard (SVM, k-ppv, etc.) sur les données transformées\n",
    "\n",
    "Tutoriel auto-encoder en Keras : https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "## Possibilité 2 : Augmenter le nombre de données\n",
    "- en bruitant les données\n",
    "- en faisant de petites modifications des données\n",
    "- d'autres idées par ici : http://leon.bottou.org/projects/infimnist\n",
    "Puis réapprenez le réseau.\n",
    "\n",
    "## Possibilité 4 : Fashion-MNIST\n",
    "Apprendre un réseau de neuronnes réalisant la classification 10 classes sur les données de l'entreprise de fringues Zalando : https://github.com/zalandoresearch/fashion-mnist \n",
    "\n",
    "## Possibilité 5 : Autre architecture de réseau\n",
    "L'architecture que nous avons utilisé est trop simple : 3 couches denses et c'est tout. L'idée ici est de prendre un résau plus complexe, avec des couches de convolution, du pooling, du dropout. L'utilisation de la partie fonctionnelle de Keras semble obligatoire (https://keras.io/getting-started/functional-api-guide/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

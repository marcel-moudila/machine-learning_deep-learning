{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligence Artificielle Avancée\n",
    "## TP5 : Introduction au Deep Learning\n",
    "\n",
    "Au cas où, il nous faut d'abord vérifier la version du Keras et Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "print(\"Keras:\", keras.__version__)\n",
    "print(\"Tensorflow:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installer Pydot et Graphiz, si ne sont pas déjà installés:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Si vous utilisez Anaconda, tapez: \"!conda install pydot\" sinon, \"!pip install pydot\" \n",
    "Ou vous pouvez également installer Pydot à l'aide de l'interface graphique Anaconda:\n",
    "Menu--> Environnements-->Not installed, puis cherchez pydot, cliquez \"Apply\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis vous devez relancer le noyau : trouver 'restart kernel' dans les menus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De la documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning avec Keras\n",
    "- Doc keras :https://keras.io/\n",
    "- Le model Sequential (premier pas) : https://keras.io/getting-started/sequential-model-guide/#getting-started-with-the-keras-sequential-model\n",
    "- Un framework plus riche : https://keras.io/getting-started/functional-api-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autres Toolkits et packages\n",
    "\n",
    "- Lasagne : Langage *de haut niveau* comme keras\n",
    "\n",
    "\n",
    "NB : Lasagne et Keras utilisent indifférement un backend parmi Theano (Univ. Montreal) et Tensorflow (Google)\n",
    "\n",
    "- Theano :  le package de l'Univ. de Montréal\n",
    "\n",
    "- Tensorflow :  le package Google\n",
    "\n",
    "- Caffe : très spécialisé images\n",
    "\n",
    "- Pytorch : la plateforme de Facebook\n",
    "\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération d'un dataset existant\n",
    "On va travailler avec un jeux de données historique que vous conaissez déjà : MNIST. Ce sont des chiffres manuscrits, donc des images en niveaux de gris, en résolution 28x28 cette fois (cela peut prendre un peu de temps - et générer un FutureWarning, ce qui n'est pas grave) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"dimension des données : \", X_train.shape, y_train.shape)\n",
    "print(\"une données : \")\n",
    "print(X_test[42])\n",
    "\n",
    "print(\"Une données sous format plus lisible :\")\n",
    "plt.imshow(X_train[42], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitement\n",
    "Les données ne sont pas dans le format nécessaire à Keras : il vaut vectoriser chaque image. On va aussi les normaliser, pour n'avoir que des valeurs comprisent entre 0 et 1 (améliore la vitesse de convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "import numpy as np\n",
    "X_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
    "X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(\"Classe de la data num 42 :\", y_train[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot-encoding\n",
    "Les réseaux de neurones ont besoin d'un vecteur à la place de *y* : on utilise un codage où la taille de ce vecteur est le nombre de classe, toutes les valeurs du vecteurs sont égales à 0, sauf celle qui correspond à la bonne classe qui elle vaut 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On transforme les sorties (numéros de classe) en des vecteurs de type one-hot-code\n",
    "nb_classes = 10\n",
    "\n",
    "Y_train = y_train\n",
    "Y_test = y_test\n",
    "y_train =  np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(\"Classe de la data num 42 : \", y_train[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprendre un réseau de neurones pour la classification de MNIST\n",
    "D'abord il faut créer le model.\n",
    "On ajoute les couches une à une. Ici un modèle qui :\n",
    "- prend en entrée un vecteur de dimension 784 (une image Mnist 28x28 vectorisée)\n",
    "- transforme l'entrée en un vecteur de dimension 64 avec une couche totalement connectée (Dense), avec une fonction d'une activation de type Rectified Linear Unit \n",
    "- Transforme la sortie de la couche précédente (de dimension 64) en un vecteur de dimension 10 avec une autre couche dense \n",
    "- Transforme le vecteur de dimenbsion 10 en un autre vecteur de dimension 10 à l'aide d'une couche dense avec la fonction d'activation softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from Keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée note réseau de neurones (cela génère un warning désagréable mais qui ne pause pas de problème finalement) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=784, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis il faut \"compiler\" le modèle, en précisant :\n",
    "- le critère d'optimisation : le *loss*\n",
    "- la routine d'optimisation (ie l'utilisation du gradient) : l'*optimizer*\n",
    "- les métriques additionnelles au *loss* (ici l'*accuracy*, le taux de bonne classification) que l'on va calculer à chaque fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage de la structure du modèle\n",
    "\n",
    "Expliquee les différentes éléments de chacune des lignes affichées par la commande suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou une manière plus graphique (nécessite l'installation de pydot et surtout de graphviz, ce qui n'est pas forcément facile) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On peut maintenant apprendre le modèle en précisant \n",
    "- la base d'apprentissage (les deux premiers paramètres)\n",
    "- le nombre d'itérations d'apprentissage (*epochs*)\n",
    "- la taille des minibatchs (*batch_size*)\n",
    "- un ensemble de validation (soit on utilise comme ici un pourcentage des données d'entrée *X_train, y_train*, soit d'autres ensemble de données via *validation_data=(X_test, y_test)*)\n",
    "- le niveau de verbosité de l'affichage\n",
    "\n",
    "*(là encore, on a un WARNING, mais il n'empêche pas le code de fonctionner)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit(X_train, y_train,\n",
    "              epochs=3,\n",
    "              batch_size=16,\n",
    "              verbose =1,\n",
    "              validation_split=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "On peut alors évaluer le modèle sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 - 0s - loss: 2.1733 - acc: 0.2016\n",
      "[2.1733250617980957, 0.20160000026226044]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=2, batch_size=16)\n",
    "\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historique de processus\n",
    "Les info sur le processus d'apprentissage sont stockées dans *h*.\n",
    "\n",
    "Comme on va en avoir besoin plusieurs fois, on écrit une fonction qui prend un historique d'apprentissage et affiche les courbes : une pour la fonction de perte (loss) et l'autre pour le taux de réussite (accuracy) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affiche_evolution_apprentissage(history):\n",
    "    #affiche history.history.keys()\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('accuracy du modèle')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['données apprentissage', 'données test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # résumé de l'historique pour loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('loss du modèle')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['apprentissage', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affiche_evolution_apprentissage(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En utilisant les réseaux de neurones convolutionnels, proposer un deuxième modèle sur la même donnée (MNIST), le modèle proposé sera constité de deux couches (Conv2D de taille 3), après chaque couche de convoltion une couche d'échantillonnage  (MaxPooling2D de taille 2) est fortement recommandée.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A vous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluer les nouveau model construit (CNN) et le comparer avec le modèle dense (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a vous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluer les hyperparamètres droupout et batch-size sur la perfermance du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A vous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback\n",
    "Permet de programmer la sauvegarde des modèles à chaque itération, l'adaptation du pas d'apprentissage (learning rate), une procédure de early stopping, etc.\n",
    "\n",
    "Voir <https://keras.io/callbacks/> pour les détails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "# Adaptation du pas d'apprentissage\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=0, mode='auto', \n",
    "                       min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "h_es_lr = model.fit(X_train, y_train,\n",
    "                    epochs=2,\n",
    "                    batch_size=16,\n",
    "                    verbose =1,\n",
    "                    validation_split=0.33,\n",
    "                    callbacks=[es, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauver et récupérer des models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Eventuellement, installation d'un module nécessaire :\n",
    "#!pip3 install h5py\n",
    "\n",
    "score = model.evaluate(X_test, y_test, batch_size=16)\n",
    "print (\"Initialement : \", score)\n",
    "\n",
    "# Sauver le model \n",
    "model.save('mon_modele.h5')  # crée un fichier HDF5del model  \n",
    "\n",
    "# supprime le modèle\n",
    "del model\n",
    "\n",
    "# Récupérer le modèle   \n",
    "model = load_model('mon_modele.h5')\n",
    "\n",
    "score = model.evaluate(X_test, y_test, batch_size=16)\n",
    "print (\"Après suppression-récupération : \", score)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser Keras dans un code SciKit-learn (cross-validation / Grid search)\n",
    "\n",
    "Dans Scikit-Learn, il y a une classe *KerasClassifier* qu'il peut-être utile de savoir utiliser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_model_Mnist(optimizer='rmsprop', input_datadim = 784, init='glorot_uniform', nb_hid1= 20, do_rate= 0.5 ):\n",
    "    # fonction créant un modèle pour MNIST\n",
    "    \"\"\"\n",
    "    #Jusqu'à présent :\n",
    "    m = Sequential()\n",
    "    m.add(Dense(nb_hid1, input_dim=input_datadim, activation='relu'))\n",
    "    m.add(Dropout(do_rate))\n",
    "    m.add(Dense(64, activation='relu'))\n",
    "    m.add(Dropout(do_rate))\n",
    "    m.add(Dense(10, activation='softmax'))\n",
    "    \"\"\"\n",
    "    # De façon équivalente :\n",
    "    entree= Input(shape=(784,))\n",
    "    cachee_1= Dense(64, activation='relu')(entree)\n",
    "    cachee_2 = Dense(64, activation='relu')(cachee_1)\n",
    "    sortie = Dense(10, activation=\"softmax\")(cachee_2)\n",
    "    m = Model(entree, sortie)\n",
    "    \n",
    "    m.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    m.summary()\n",
    "    return m\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model_Mnist)\n",
    "\n",
    "# valeurs des différents paramètres\n",
    "optimizers = ['adam', 'rmsprop']\n",
    "init = ['glorot_uniform']\n",
    "V_nb_hid1 = [100]\n",
    "DO_rate=[0, 0.5]\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "param_grid = dict(optimizer=optimizers, init=init,  nb_hid1=V_nb_hid1, do_rate= DO_rate)\n",
    "\n",
    "print(param_grid)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, epochs = epochs, verbose=2)\n",
    "\n",
    "# Résumé des résultats\n",
    "print(\"-----------------------------\")\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des vecteurs d'activation d'une couche\n",
    "Une fois un réseau appris, on peut avoir besoin des vecteurs de sortie d'une couche cachée. Avec Keras, il suffit de créer un nouveau modèle qui ne contient que le début du réseau jusqu'à la couche dont on veut accéder aux sorties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#réseau complet\n",
    "entree= Input(shape=(784,))\n",
    "cachee_1= Dense(64, activation='relu')(entree)\n",
    "cachee_2 = Dense(64, activation='relu')(cachee_1)\n",
    "sortie = Dense(10, activation=\"softmax\")(cachee_2)\n",
    "m = Model(entree, sortie)\n",
    "\n",
    "#apprentissage\n",
    "m.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "m.fit(X_train, y_train, epochs=2, batch_size=16, verbose =1, validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 64)\n"
     ]
    }
   ],
   "source": [
    "#réseau partiel\n",
    "m2 = Model(entree,cachee_1)\n",
    "\n",
    "#On aurait aussi pu faire :\n",
    "# m2 = Sequential()\n",
    "# m2.add(Dense(64,input_dim=784, activation='relu', weights=m.layers[0].get_weights()))\n",
    "\n",
    "#récupération d'une matrice d'activation :\n",
    "#chaque ligne est le vecteur de sortie de la dernière couche de m2 pour la donnée correspondante\n",
    "activite = m2.predict(X_test)\n",
    "print(activite.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A faire\n",
    "\n",
    "## Apprentissage d'un réseau de neurones sur les données iris\n",
    "\n",
    "\n",
    "Apprendre un réseau de neuronnes réalisant la classification 3 classes sur les données iris.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**. En utilisant Pandas, lire les données iris à partir de la bibliothèque Sklearn et compléter les instructions ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Lire les données iris\n",
    "iris = load_iris()\n",
    "\n",
    "\n",
    "# Charger des données dans un DataFrame\n",
    "\n",
    "#..........Ecrire votre instruction ici............\n",
    "\n",
    "\n",
    "# Convertir le type de données en float\n",
    "\n",
    "#..........Ecrire votre instruction ici............\n",
    "\n",
    "\n",
    "# ajoutez \"target\" dans le DataFrame et nommez-le \"label\"\n",
    "\n",
    "#..........Ecrire votre instruction ici............\n",
    "\n",
    "# Utilisez plutôt une étiquette de chaîne\n",
    "df['label'] = df.label.replace(dict(enumerate(iris.target_names)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Afficher le contenu du DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Transformer les labels de classes en des vecteurs de type one-hot-code et supprimer l'ancien label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Ré-afficher le contenu du nouveau DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Donner une petite explication sur la différence entre l'ancien DatFrame et celui-ci."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Extraire les données x (samples) et y (classes) à partir des données iris, en les convertant en Numpy pour faire la classification (En utilisant toujours Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Découper les données en ensembles de training et test (20 % pour le test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Construire un modèle du réseau de neurones séquentiel: 3 couches cachées (64, 128, 64) et les fonctions d'activation sont relu() et softmax()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Visualiser l''arborescence du modèle construit. Commenter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Compiler et entraîner votre modèle avec le nombre d'epochs=15, l'ensemble de validation=25% et  batch size=40. Vous pouvez utiliser Adam comme optimiseur. A vous de choisir une fonction et métrique de perte adéaquates  pour ce type de problème. Réflichissez!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Tracez les courbes d'apprentissage (train_loss et val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Interpréter le résultat obtenu. Que constatez-vous ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Après avoir construit le modèle, utiliser la commande  EarlyStopping() sans paramètres sous Keras pour éviter le problème d'Overfitting. Re-entraîner le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.**  Interpréter le résulat"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Re-tracez les courbes d'apprentissage (train_loss et val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.**  Interpréter le résulat des courbes d'apprentissage obtenus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Après avoir essayer EarlyStopping() sans paramètres, personaliser maintenant cette commande en fixant les les paramètres min_delta=$1e-3$, patience=8 et mode='max'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Expliquer les paramètres suivants: min_delta, patience et verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Entraîner votre modèle en tenant en compte l'instruction d'EarlySopping personalisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Interpréter le résultat"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Tracez les courbes d'apprentissage (train_loss et val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Que constatez-vous ?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression avec EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir apprendre un modèle de classification, maintenant on s'intéresse à un problème de régression. \n",
    "\n",
    "En utilisant Pandas, nous allons charger le fichier CSV des données auto à partir de cette URL: \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\". Dans la même instruction, nous vérifiions s'il y a des valeurs manquantes comme 'NA' ou '?' et les stocker dans une variable nommée na_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "cars = df['name']\n",
    "\n",
    "# Gérer les valeurs manquantes\n",
    "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Convertir les données en Numpy pour faire la régression (données x et y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Découper les données en ensembles de training et test avec la taille de test=25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Constuire un modèle de réseau de neurones séquentiel contenant 2 couches cachées (25, 10). Attention: Il faut choisir la métrique de perte la plus adéquate! Vous pouvez utiliser ADAM comme l'exercice précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Entraîner maintenant le modèle avec un nombre d'epochs=300. Vous ajouterez la méthode EarlyStopping() avec min delta=$1e-3$, patience=5 et mode='auto'. Afficher et commenter les résultats obtenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
